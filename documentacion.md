## üìä An√°lisis inicial de .describe().T` del dataset `Customer Flight Activity`

La exploraci√≥n con `df_flight.describe().T` aporta informaci√≥n muy √∫til para entender la distribuci√≥n general de las variables num√©ricas del dataset.

---------------------

### ‚úÖ Principales insights detectados:

#### 1. Distribuciones generales
- **Flights Booked**: Media = 4.12, Mediana = 1.00 ‚Üí Distribuci√≥n asim√©trica. La mayor√≠a de clientes reserva pocos vuelos, pero hay algunos con muchos vuelos (m√°x: 21).
- **Flights with Companions**: Similar patr√≥n, con una media baja (1.03) y m√°ximos altos (hasta 11).
- **Points Redeemed** y **Dollar Cost Points Redeemed**: Tienen medias bajas pero desviaciones est√°ndar muy altas -> algunos clientes redimen much√≠simos puntos.

#### 2. Valores cero frecuentes
- Varias columnas como `Flights Booked`, `Distance`, `Points Redeemed` y `Dollar Cost Points Redeemed` tienen **m√≠nimos en 0**, lo que puede reflejar meses sin actividad para algunos clientes.

#### 3. Rango y outliers
- Algunas columnas muestran valores extremos, como:
  - `Points Redeemed`: hasta 876 puntos redimidos.
  - `Dollar Cost Points Redeemed`: m√°ximo de 71$, muy por encima del percentil 75 (que es 0.00).

#### 4. Validaci√≥n de datos de fecha
- `Year`: media = 2017.5, m√≠nimo = 2017, m√°ximo = 2018 ‚Üí confirman que los datos corresponden a solo dos a√±os.
- `Month`: de 1 a 12, con media de 6.

---------------------------

## ‚ÑπÔ∏è Exploraci√≥n con `.info()` del dataset `Customer Flight Activity`

La funci√≥n `df_flight.info()` nos proporciona una visi√≥n general de la estructura del dataset. A continuaci√≥n se detallan los principales puntos observados:

---

### ‚úÖ Tama√±o del dataset
- **405.624 filas** y **10 columnas** ‚Üí un dataset amplio, con un buen volumen de informaci√≥n para analizar.

### ‚úÖ Valores nulos
- Todas las columnas tienen **405.624 valores no nulos**, por lo tanto:
  - **No hay valores nulos** en este dataset.
  - No se requiere tratamiento de nulos en esta fase.

### ‚úÖ Tipos de datos (`dtype`)
- **9 columnas** tienen tipo de dato `int64`.
- **1 columna** (`Points Accumulated`) es `float64`, lo cual es l√≥gico ya que puede tener valores decimales.
- Confirmamos que los decimales est√°n representados correctamente con `punto (.)`, como espera Python.

### ‚úÖ Memoria
- El DataFrame ocupa aproximadamente **30.9 MB en memoria**, lo cual es razonable para su tama√±o.

---

### üß† Conclusi√≥n
Los datos est√°n **completos y correctamente tipados**, lo cual nos permite pasar sin problemas a la siguiente fase: exploraci√≥n visual, an√°lisis de valores extremos o uni√≥n con otros datasets.

----------------------------------

## üìë Duplicados en el dataset `Customer Flight Activity`

Se detectaron **1864 filas duplicadas**.  
Esto significa que existen **1864 registros que son id√©nticos en todas las columnas**.

---

### ‚úÖ Opciones para tratar los duplicados

#### 1. Mantener los duplicados
- Puede ser v√°lido si los datos duplicados reflejan situaciones reales repetidas (por ejemplo, vuelos m√∫ltiples del mismo tipo).
- Aun as√≠, es recomendable dejar constancia de que los duplicados existen y han sido analizados.

#### 2. Eliminar los duplicados
- Si se asume que estas repeticiones no aportan valor o distorsionan el an√°lisis estad√≠stico, pueden eliminarse f√°cilmente con:

---

*** An√°lisis detallado de filas duplicadas ***

Uso  df_flight[df_flight.duplicated()].value_counts() para identificar exactamente cu√°les son las filas duplicadas dentro del dataset Customer Flight Activity.

- Resultado observado:

Se identificaron 1848 combinaciones de filas duplicadas, lo que representa un total de 1864 filas repetidas.
Estas duplicaciones afectan a m√∫ltiples clientes, no solo a uno.
La mayor√≠a de estas filas contienen valores en cero en todas las columnas de actividad:
Flights Booked = 0
Distance = 0
Points Accumulated = 0
Dollar Cost Points Redeemed = 0
etc.
Esto sugiere que se trata de meses sin actividad que han sido duplicados por error en el proceso de carga de datos.

# Conclusi√≥n:

Tras analizar las filas duplicadas en el dataset Customer Flight Activity, se identificaron 1864 registros repetidos exactamente (misma informaci√≥n en todas las columnas). La mayor√≠a de estas filas contienen datos con actividad nula: cero vuelos, cero puntos, cero distancia, etc.

Dado que estos duplicados:

- No aportan informaci√≥n nueva,
- Representan posibles errores de carga en meses sin actividad,
- pueden distorsionar los resultados del an√°lisis (por ejemplo, inflando los promedios o el n√∫mero total de registros por cliente),

Decido eliminarlos del dataset antes de continuar con la fase de an√°lisis y visualizaci√≥n. Esta limpieza mejora la calidad del dataset y permite obtener resultados m√°s precisos.


-----------------------------------

## üîé Diferencia entre `.nunique()` y `.unique()` en pandas

Durante la exploraci√≥n del DataFrame `df_flight`, hem utilizado el m√©todo `.nunique()` para obtener el n√∫mero de valores √∫nicos por columna.

---

### ‚úÖ `.nunique()`
- Devuelve **cu√°ntos valores √∫nicos hay** en cada columna (o en una columna espec√≠fica).
- Muy √∫til para detectar columnas con baja variabilidad o redundantes.

-----------------------

### csv Loyalty:

# .describe()

An√°lisis del resumen estad√≠stico (.describe().T) del dataset Customer Loyalty History
La funci√≥n df_loyalty.describe().T proporciona estad√≠sticas b√°sicas de las columnas num√©ricas. A continuaci√≥n se detallan los principales hallazgos:

1. Salary

Solo hay datos de salario para 12.499 clientes, lo que indica que faltan m√°s de 4.000 valores ‚Üí se deber√° tratar como valor nulo.
La media salarial es de 79.245‚ÄØ$, pero hay valores negativos (m√≠nimo: -58.486‚ÄØ$), lo cual no tiene sentido y debe considerarse un error.
El m√°ximo es de 407.228‚ÄØ$, lo que podr√≠a indicar un outlier o cliente de muy alto ingreso.

Tratamiento de salarios negativos

Durante la revisi√≥n de la columna Salary, se detectaron 20 valores negativos.
Aunque no es un n√∫mero elevado, se considera que estos valores corresponden a errores de carga, ya que el salario no puede ser negativo en un contexto realista.

Como en ejercicios anteriores se han tratado como valores absolutos, se decide mantener esa misma l√≥gica aqu√≠.

Adem√°s:

El n√∫mero de casos es muy bajo (20 de 16.737 registros), por lo que no distorsionar√° las estad√≠sticas.
As√≠ se evita eliminar informaci√≥n √∫til de esos clientes.
Decisi√≥n: transformar esos valores negativos a positivos utilizando .abs().


2. Cancellation Year y Cancellation Month

Solo 2.067 clientes tienen una fecha de cancelaci√≥n registrada.
Esto indica que la mayor√≠a siguen activos.
Las fechas y meses son correctos y no presentan errores aparentes.


# .info()

‚ÑπÔ∏è An√°lisis del .info() del dataset Customer Loyalty History
Al ejecutar df_loyalty.info(), se obtiene una visi√≥n general de la estructura y calidad de los datos. A continuaci√≥n se resumen los puntos clave:

1. Detecci√≥n de valores nulos

Aunque .info() no muestra expl√≠citamente los valores "NaN", se puede detectar su presencia comparando los valores en la columna Non-Null Count.
Salary: tiene solo 12.499 valores no nulos, por lo tanto hay 4.238 valores nulos.
Cancellation Year y Cancellation Month: tienen 2.067 valores no nulos, lo que indica que la mayor√≠a de los clientes no han cancelado su membres√≠a.

3. Tipos de datos (dtype)

Cancellation Year y Cancellation Month: aparecen como float64 porque contienen valores nulos. Aunque sem√°nticamente son enteros, pandas los convierte a float cuando hay nulos.
Si en alg√∫n momento se imputan o eliminan los nulos, se podr√≠an convertir a tipo entero usando Int64 (con soporte para nulos).

Conclusi√≥n:
Los datos est√°n en general bien tipados y organizados. Se identifican valores nulos relevantes en Salary y en las columnas de cancelaci√≥n, lo que deber√° ser tratado durante la limpieza. El resto de columnas presentan tipos adecuados para su an√°lisis posterior.

# .nunique() 

An√°lisis de valores √∫nicos con .nunique()
Se ejecut√≥ df_loyalty.nunique() para conocer cu√°ntos valores √∫nicos existen por columna en el dataset. Este an√°lisis permite identificar columnas con poca variaci√≥n, posibles identificadores √∫nicos, y variables √∫tiles para segmentar o agrupar.

1. Country ‚Üí 1 √∫nico valor. Todos los clientes pertenecen al mismo pa√≠s. Esta columna no aporta valor anal√≠tico y puede eliminarse.

Conclusi√≥n:
La mayor√≠a de columnas muestran una variabilidad adecuada para el an√°lisis.  Eliminar la columna Country (sin variaci√≥n), y revisar si Postal Code aporta valor adicional respecto a ciudad y provincia.