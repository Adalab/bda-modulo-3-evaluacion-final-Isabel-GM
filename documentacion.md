## Documentacion:

---

##  Principales insights detectados:

# 1. Distribuciones generales
- Flights Booked: Media = 4.12, Mediana = 1.00 ‚Üí Distribuci√≥n asim√©trica. La mayor√≠a de clientes reserva pocos vuelos, pero hay algunos con muchos vuelos (m√°x: 21).
- Flights with Companions: Similar patr√≥n, con una media baja (1.03) y m√°ximos altos (hasta 11).
- Points Redeemed y Dollar Cost Points Redeemed: Tienen medias bajas pero desviaciones est√°ndar muy altas -> algunos clientes redimen much√≠simos puntos.

# 2. Valores cero frecuentes
- Varias columnas como `Flights Booked`, `Distance`, `Points Redeemed` y `Dollar Cost Points Redeemed` tienen **m√≠nimos en 0**, lo que puede reflejar meses sin actividad para algunos clientes.

# 3. Rango y outliers
- Algunas columnas muestran valores extremos, como:
  - `Points Redeemed`: hasta 876 puntos redimidos.
  - `Dollar Cost Points Redeemed`: m√°ximo de 71$, muy por encima del percentil 75 (que es 0.00).

# 4. Validaci√≥n de datos de fecha
- `Year`: media = 2017.5, m√≠nimo = 2017, m√°ximo = 2018 ‚Üí confirman que los datos corresponden a solo dos a√±os.
- `Month`: de 1 a 12, con media de 6.

---

###  Exploraci√≥n con `.info()` del dataset `Customer Flight Activity`

La funci√≥n `df_flight.info()` proporciona una visi√≥n general de la estructura del dataset. Detalles de la info obtenida:

# ‚úÖ Valores nulos
- Todas las columnas tienen **405.624 valores no nulos**, por lo tanto:
  - **No hay valores nulos** en este dataset.
  - No se requiere tratamiento de nulos en esta fase.

# ‚úÖ Tipos de datos (`dtype`)
- **9 columnas** tienen tipo de dato `int64`.
- **1 columna** (`Points Accumulated`) es `float64`, lo cual es l√≥gico ya que puede tener valores decimales.
- Confirmo que los decimales est√°n representados correctamente con `punto (.)`, como espera Python.

Los datos est√°n **completos y correctos**, lo cual permite pasar sin problemas a la siguiente fase de exploraci√≥n visual, an√°lisis de valores extremos o uni√≥n con otros datasets.

----------------------------------

## üìë Duplicados en el dataset `Customer Flight Activity`

Se detectan **1864 filas duplicadas**.  
Esto significa que existen **1864 registros que son id√©nticos en todas las columnas**.

## ‚úÖ Opciones para tratar los duplicados

# 1. Mantener los duplicados
- Puede ser v√°lido si los datos duplicados reflejan situaciones reales repetidas (por ejemplo, vuelos m√∫ltiples del mismo tipo).
- Aun as√≠, dejo constancia de que los duplicados existen y han sido analizados.

# 2. Eliminar los duplicados
- Si se asume que estas repeticiones no aportan valor o distorsionan el an√°lisis estad√≠stico, pueden eliminarse f√°cilmente.


*** An√°lisis detallado de filas duplicadas ***

Uso  df_flight[df_flight.duplicated()].value_counts() para identificar exactamente cu√°les son las filas duplicadas dentro del dataset Customer Flight Activity.

- Resultado observado:

Se identificaron 1848 combinaciones de filas duplicadas, lo que representa un total de 1864 filas repetidas.
Estas duplicaciones afectan a m√∫ltiples clientes, no solo a uno.
La mayor√≠a de estas filas contienen valores en cero en todas las columnas de actividad:
Flights Booked = 0
Distance = 0
Points Accumulated = 0
Dollar Cost Points Redeemed = 0
etc.
Esto sugiere que se trata de meses sin actividad que han sido duplicados por error en el proceso de carga de datos.

# Conclusi√≥n:

Tras analizar las filas duplicadas en el dataset Customer Flight Activity, se identificaron 1864 registros repetidos exactamente (misma informaci√≥n en todas las columnas). La mayor√≠a de estas filas contienen datos con actividad nula: cero vuelos, cero puntos, cero distancia, etc.

Dado que estos duplicados:

- No aportan informaci√≥n nueva,
- Representan posibles errores de carga en meses sin actividad,
- pueden distorsionar los resultados del an√°lisis (por ejemplo, inflando los promedios o el n√∫mero total de registros por cliente),

Decido eliminarlos del dataset antes de continuar con la fase de an√°lisis y visualizaci√≥n. Esta limpieza mejora la calidad del dataset y permite obtener resultados m√°s precisos.


---

### csv Loyalty:

# .describe()

An√°lisis del resumen estad√≠stico (.describe().T) del dataset Customer Loyalty History
La funci√≥n df_loyalty.describe().T proporciona estad√≠sticas b√°sicas de las columnas num√©ricas. A continuaci√≥n se detallan los principales hallazgos:

1. Salary

Solo hay datos de salario para 12.499 clientes, lo que indica que faltan m√°s de 4.000 valores ‚Üí se deber√° tratar como valor nulo.
La media salarial es de 79.245‚ÄØ$, pero hay valores negativos (m√≠nimo: -58.486‚ÄØ$), lo cual no tiene sentido y debe considerarse un error.
El m√°ximo es de 407.228‚ÄØ$, lo que podr√≠a indicar un outlier o cliente de muy alto ingreso.

***  Tratamiento de salarios negativos ***

Durante la revisi√≥n de la columna Salary, se detectaron 20 valores negativos.
Aunque no es un n√∫mero elevado, se considera que estos valores corresponden a errores de carga, ya que el salario no puede ser negativo en un contexto realista.

Como en ejercicios anteriores se han tratado como valores absolutos, se decide mantener esa misma l√≥gica aqu√≠.

Adem√°s:

El n√∫mero de casos es muy bajo (20 de 16.737 registros), por lo que no distorsionar√° las estad√≠sticas.
As√≠ se evita eliminar informaci√≥n √∫til de esos clientes.
Decisi√≥n: transformar esos valores negativos a positivos utilizando .abs().

*** Gestion de nulos de Salary ***

Tras analizar la distribuci√≥n de la columna Salary, observo que:

- La media es 79.359‚ÄØ‚Ç¨ y la mediana es 73.455‚ÄØ‚Ç¨.
- Hay cierta dispersi√≥n (desviaci√≥n t√≠pica de 34.750‚ÄØ‚Ç¨).
- El valor m√°ximo es muy elevado (407.228‚ÄØ‚Ç¨), lo que sugiere posibles outliers.

Decido rellenar los valores nulos con la mediana, ya que:
- Es una medida robusta frente a valores extremos.
- Refleja mejor el comportamiento t√≠pico del salario en este contexto.

--- 

# Gesti√≥n de nulos de las columnas Cancellation Year y Cancellation Month: 

Solo 2.067 clientes tienen una fecha de cancelaci√≥n registrada.
Esto indica que la mayor√≠a siguen activos.
Las fechas y meses son correctos y no presentan errores aparentes.

Estas columnas indican cuando el cliente dej√≥ su membres√≠a, por lo que si un cliente sigue activo, no tiene fecha de cancelacion y aparece como NaN

- ¬øQu√© hacer con estos nulos? En este caso, no se deben imputar ni eliminar, porque:
    1. Su ausencia tiene un significado: el cliente sigue siendo parte del programa.
    2. Eliminarlos ser√≠a perder informaci√≥n √∫til.
    3. Imputarlos ser√≠a inventar algo que no ocurri√≥

# üßæ Nueva columna: `Active Customer`

Se ha creado una nueva columna llamada `Active Customer` para indicar si un cliente sigue activo o no en el programa de fidelizaci√≥n.

- Se basa en la columna `Cancellation Year`:
  - Si el valor es `NaN`, significa que el cliente **sigue activo**.
  - Si hay un a√±o registrado, indica que **el cliente ha cancelado** su membres√≠a.

- Se han utilizado los valores `"Yes"` para clientes activos y `"No"` para clientes inactivos.

---

# .info()

*** ‚ÑπÔ∏è An√°lisis del .info() del dataset Customer Loyalty History ***

1. Detecci√≥n de valores nulos

Aunque .info() no muestra expl√≠citamente los valores "NaN", se puede detectar su presencia comparando los valores en la columna Non-Null Count.
Salary: tiene solo 12.499 valores no nulos, por lo tanto hay 4.238 valores nulos.
Cancellation Year y Cancellation Month: tienen 2.067 valores no nulos, lo que indica que la mayor√≠a de los clientes no han cancelado su membres√≠a.

3. Tipos de datos (dtype)

Cancellation Year y Cancellation Month: aparecen como float64 porque contienen valores nulos. Aunque sem√°nticamente son enteros, pandas los convierte a float cuando hay nulos.
Si en alg√∫n momento se imputan o eliminan los nulos, se podr√≠an convertir a tipo entero usando Int64 (con soporte para nulos).

Conclusi√≥n:
Los datos est√°n en general bien tipados y organizados. Se identifican valores nulos relevantes en Salary y en las columnas de cancelaci√≥n, lo que deber√° ser tratado durante la limpieza. El resto de columnas presentan tipos adecuados para su an√°lisis posterior.


 *** Tipo de dato en columnas de cancelaci√≥n ***

Durante la exploraci√≥n inicial las columnas `Cancellation Year` y `Cancellation Month` aparecen como tipo `float64`, a pesar de que representan a√±os y meses (valores enteros).


 ¬øPor qu√© son `float64` y no `int64`?

Esto ocurre porque ambas columnas contienen valores `NaN`, los cuales pandas solo puede almacenar en columnas de tipo `float` (en su comportamiento por defecto).  
Los tipos `int` normales (`int64`) **no pueden contener `NaN`**, por lo que se convierte todo a `float64` autom√°ticamente.


Decido **mantener ambas columnas como `float64`**, ya que:

- Los `NaN` tienen un significado importante (clientes activos).
- No es necesario cambiar el tipo para el an√°lisis actual.
- Podria usar un tipo especial llamado "Int64" (con may√∫scula), que s√≠ acepta NaN pero forzar la conversi√≥n a `int` podr√≠a provocar errores o p√©rdida de informaci√≥n.

Si en un futuro se desea trabajar con enteros y permitir `NaN`, podr√≠a considerarse convertirlas a `Int64` (entero "nullable" de pandas).


# .nunique() 

An√°lisis de valores √∫nicos con .nunique()
Se ejecut√≥ df_loyalty.nunique() para conocer cu√°ntos valores √∫nicos existen por columna en el dataset. Este an√°lisis permite identificar columnas con poca variaci√≥n, posibles identificadores √∫nicos, y variables √∫tiles para segmentar o agrupar.

1. Country ‚Üí 1 √∫nico valor. Todos los clientes pertenecen al mismo pa√≠s. Esta columna no aporta valor anal√≠tico y puede eliminarse.

Conclusi√≥n:
La mayor√≠a de columnas muestran una variabilidad adecuada para el an√°lisis.  Eliminar la columna Country (sin variaci√≥n), y revisar si Postal Code aporta valor adicional respecto a ciudad y provincia.


---

### üîó Uni√≥n de los datasets: vuelos + informaci√≥n de cliente

Decidido limpiar primero cada dataset por separado (df_loyalty y df_flight) antes de unirlos, con el objetivo de:

- Corregir valores err√≥neos (como salarios negativos).
- Imputar nulos con la l√≥gica adecuada seg√∫n cada contexto.
- Eliminar duplicados exactos que no aportaban informaci√≥n.

As√≠ aseguro que el merge se hace sobre datos limpios y consistentes, evitando arrastrar errores al DataFrame final.

Para combinar la informaci√≥n de comportamiento de vuelo (`df_flight`) con los datos personales y demogr√°ficos (`df_loyalty`), realizo una uni√≥n mediante la columna com√∫n `Loyalty Number`.


---

### VISUALIZACION: 

# 1. ¬øComo se distribuye la cantidad de vuelos reservados por mes durante el a√±o?

# üìä Distribuci√≥n de vuelos reservados por mes

El objetivo de esta visualizaci√≥n es analizar **c√≥mo var√≠a la cantidad de vuelos reservados (`Flights Booked`) a lo largo del a√±o**, agrupando los datos por mes.

Selecciono el gr√°fico de barras por los siguientes motivos:

- Permite **comparar cantidades entre categor√≠as** discretas (en este caso, los meses del a√±o).
- Muestra con claridad las diferencias entre los meses.


---


## 2. ¬øExiste una relaci√≥n entre la distancia de los vuelos y los puntos acumulados por los cliente?


En la visualizaci√≥n se observa una **relaci√≥n lineal positiva muy clara y marcada**.  
Los puntos siguen trayectorias diagonales bien definidas, lo cual indica que a **mayor distancia recorrida**, los clientes **acumulan m√°s puntos**.

Esta relaci√≥n es l√≥gica dentro del contexto de un programa de fidelizaci√≥n, donde los puntos suelen asignarse en funci√≥n de la distancia volada.

Existe una **correlaci√≥n directa, sistem√°tica y consistente** entre ambas variables. Esto sugiere que la asignaci√≥n de puntos por vuelo se realiza siguiendo una f√≥rmula clara y estable en funci√≥n de la distancia.

--- 

## 3. ¬øCu√°l es la distribuci√≥n de los clientes por provincia o estado?

He utilizado un gr√°fico de barras horizontales para visualizar la distribuci√≥n de clientes √∫nicos por provincia.
El gr√°fico muestra una **distribuci√≥n claramente desigual** entre las diferentes regiones. La interpretacion esta a continuaci√≥n del gr√°fico.

--- 

## 4.  ¬øC√≥mo se compara el salario promedio entre los diferentes niveles educativos de los clientes?

El gr√°fico muestra c√≥mo var√≠a el salario promedio de los clientes en funci√≥n de su nivel educativo. Se observa una **tendencia clara**: a mayor nivel educativo, mayor salario promedio.

- Los clientes con **Doctorado** son los que presentan el salario promedio m√°s alto.
- Les siguen aquellos con **Master**, tambi√©n con ingresos elevados.
- **Bachelor** y **College** muestran salarios similares, en un rango medio.
- El grupo **High School or Below** tiene el salario promedio m√°s bajo.

Esta distribuci√≥n es coherente con lo esperado, reflejando que **la formaci√≥n acad√©mica tiene un impacto positivo en los ingresos** de los clientes.

---

# 5.  ¬øCu√°l es la proporci√≥n de clientes con diferentes tipos de tarjetas de fidelidad?

Para resolver este ejercicio, tengo que tener en cuenta  que cada cliente puede aparecer m√∫ltiples veces en el DataFrame original (`df_unido`) debido al registro mensual de su actividad, sin embargo, el tipo de tarjeta de fidelidad (`Loyalty Card`) **no cambia** para un mismo cliente, por lo que **es fundamental contar cada cliente una sola vez**.

---
# 6. ¬øC√≥mo se distribuyen los clientes seg√∫n su estado civil y g√©nero?

Utilizare un countplot() agrupando por Genero (en hue= Gender) porque permte comparar los valores dentro de cada categoria (mujeres y hombres dentro de married)y tambien comparar entre ellas (solteros vs casados) y ambas son columnas categoricas 


